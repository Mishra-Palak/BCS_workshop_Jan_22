{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzbkJlK_35M0"
      },
      "source": [
        "# **##################### Problem 1 #####################**\n",
        "\n",
        "The aim of this problem is to get familiar with the equations that define RNN dynamics. All the questions, except **e** and **f**, do not require any programming but can be solved with a pencil, a paper, and a bit of thinking.\n",
        "\n",
        "We have seen during the lecture that RNN dynamics are given by\n",
        "\n",
        "$$\\frac{d x_{i}(t)}{d t}=-x_{i}(t)+\\sum_{j=1}^{N} J_{i j} \\phi(x_{j}(t))+h_{i}(t) ------ \\{1\\}.$$                                                   \n",
        "\n",
        "Variable $x_i$ represents the input current entering neuron $i$, while $\\phi(x_i)$ represents the output firing rate. $J_{ij}$ is a $N\\times N$ matrix representing recurrent connectivity, and $h_i$ is the external input received by neuron $i$.\n",
        "\n",
        "In this problem set, like in the first part of the lecture, we generate the recurrent connectivity at random from a Gaussian distribution: we set \n",
        "$$J_{ij}\\sim \\mathcal{N}\\left(0, \\frac{g^2}{N}\\right).$$\n",
        "\n",
        "In this first exercise, we also assume that neurons receive no input: $h_{i}(t)=0$.\n",
        "\n",
        "How can we study the dynamics that emerge from this set of equations, where the activity of each neuron input current variable $x_i(t)$ is strongly dependent on the behavior of many other variables?\n",
        "We follow the approach used in Dynamical Systems Theory and start our analysis by looking for special activity states, i.e. fixed points, that we indicate by $\\mathbf{x}^* = [x^*_1, \\dots, x^*_N]^T$ (written as a column vector). At the fixed points, current variables are stationary (i.e. they do not vary over time). Fixed point states thus obey:\n",
        "$$\\frac{d x_{i}}{d t}\\biggr\\rvert_{x_i=x_i^*}=0 \\implies x_i^* = \\sum_{j=1}^{N} J_{i j} \\phi(x_{j}^*)------ \\{2\\}.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zy7LEEmVnv2"
      },
      "source": [
        "**a)**  To begin with, as in the lecture, we consider a linear RNN: we take $\\phi(x)=x$. Write down the fixed point equation in this case. This equation admits a very simple fixed point $\\mathbf{x}^*$. Can you guess what this fixed point is? Do you think that this fixed point is the only one admitted by the dynamics? *Hint: this is a linear system of equations!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntlrnjnEbalC"
      },
      "source": [
        "*Ans*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vldm46cW6gB"
      },
      "source": [
        "**b)** Let's now turn to a non-linear RNN. As a first step, we take $\\phi(x)=\\tanh(x)$. Write down the fixed point equation in this case. Can you guess a simple fixed point? Is this simple fixed point guaranteed to be the only one?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0VTstlSblT6"
      },
      "source": [
        "*Ans*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDpC0-OcXp_6"
      },
      "source": [
        "**c)** Fixed points are stationary states of the dynamics: this means that if the RNN is initialized at the fixed point, it will stay there. Now assume that the network is initialized at a state $x_i(t)$ that is *close* to the fixed point: we take\n",
        "$$ x_i(t) = x_i^* + \\delta x_i(t)------\\{3\\}$$\n",
        "where $\\delta x_i(t)$ is small. Derive the dynamics that is obeyed by $\\delta x_i(t)$. Then approximate those dynamics by Taylor expanding the non-linearity $\\phi(x_i)$ close to the simple fixed point, and keep only the first-order. *Hint: the derivative of $\\phi(x)=\\tanh(x)$ is $\\phi'(x)=1-\\tanh^2(x)$!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe4OsXTsbupd"
      },
      "source": [
        "*Ans*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJhgu1THY7dX"
      },
      "source": [
        "**d)** We can now use the approximate dynamics we derived to understand the RNN behavior close to the fixed point.\n",
        "If everything went well, the approximate dynamics you obtained are linear. What is the expected qualitative behavior of the solution of these dynamics ? Is activity guaranteed to go back to the simple fixed point? *Hint: we have shown this in the lecture...* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_oZjxVhek6V"
      },
      "source": [
        "*Ans*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WYiLd1s8pwt"
      },
      "source": [
        "**e)** Linear algebra tells us that $\\delta x_i$ will diverge when there is (at least) one eigenvalue of $J_{ij}$ whose real part is larger than 1. By using your favourite programming language, generate a random RNN with $N=200$ and $g=0.8$. Then compute the eigenvalues of $J_{ij}$. How many are them? Plot the eigenvalues on a plane where the x and the y axes correspond respectively to the real and the imaginary parts of eigenvalues. What is the shape that eigenvalues span on those planes? How does $g$ control the shape, and the value of the maximal real part? *Hint: in Python, you can use the numpy.linalg.eigvals function*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1VFhPZA9jLs"
      },
      "source": [
        "**Note :** Complete the code snippets and look up the documentation if you don't understand something and lastly don't forget google , search anything you find difficult."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6T3Oome9lcf"
      },
      "outputs": [],
      "source": [
        "### Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import optimize\n",
        "from mpl_toolkits import mplot3d\n",
        "from matplotlib import rc\n",
        "#rc('text', usetex=True)\n",
        "np.random.seed(100)                 # to make the random sampling sample the same numbers every time you run it\n",
        "### Plot configuration\n",
        "\n",
        "fig_width = 4.2 # width in inches\n",
        "fig_height = 3.  # height in inches\n",
        "fig_size =  [fig_width,fig_height]\n",
        "plt.rcParams['figure.figsize'] = fig_size\n",
        "plt.rcParams['figure.autolayout'] = True\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "#### #### #### #### #### #### ####\n",
        "#### Generate network parameters\n",
        "### START CODE HERE ###\n",
        "\n",
        "N =                #Enter Network size \n",
        "g =               #Enter Connectivity strength\n",
        "\n",
        "# Connectivity\n",
        "\n",
        "# Initialize J (the connectivity matrix) from a gaussian distribution \n",
        "J = \n",
        "\n",
        "# calculate eigen values of the matrix J\n",
        "eig = \n",
        "### END CODE HERE ###\n",
        "\n",
        "#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### ####\n",
        "#### Plot eigenspectrum of J\n",
        "\n",
        "fg = plt.figure()\n",
        "ax0 = plt.axes(frameon=True)\n",
        "\n",
        "line, = plt.plot([1,1], [-1,1], '--', color = '0.3')\n",
        "\n",
        "plt.plot(eig.real, eig.imag, 'o', color='#CC0000', markersize = 2.5, alpha = 0.7)\n",
        "\n",
        "# Plot a circle of radius g\n",
        "\n",
        "theta = np.linspace(0, 4*np.pi, N)\n",
        "plt.plot(g*np.cos(theta), g*np.sin(theta), 'k')\n",
        "\n",
        "plt.xlim(-1,2)\n",
        "plt.ylim(-1,1)\n",
        "plt.yticks([-1,0,1])\n",
        "\n",
        "plt.xlabel('$\\mathcal{R}(\\lambda)$')\n",
        "plt.ylabel('$\\mathcal{I}(\\lambda)$')\n",
        "\n",
        "plt.locator_params(nbins=4)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1CIs3zyaOxX"
      },
      "source": [
        "**f)** We have learnt from the previous point that approximately when $g>1$, activity diverges away from the simple fixed point.\n",
        "In that case, we say that the fixed point is *unstable*, we don't know apriori what will happen: Dynamical Systems Theory indicates that other fixed points, limit cycles, or chaotic attractors might emerge. Understanding the behavior of random RNNs in this case is the focus of the next exercise.\n",
        "\n",
        "We consider here a different issue.\n",
        "In the RNN we just examined, we assumed that $\\phi(x)=\\tanh(x)$. This was in fact a very lucky choice, because we were able to easily guess the value of the fixed point that is admitted by the dynamics, and evaluate activity evolution around it. In RNNs characterized by a general activation function, finding the fixed points can be hard, because easy guesses do not work. For example, we will use an activation function that generates only positive firing rates:\n",
        "$$\\phi(x)=\\frac{1}{2}\\left(\\tanh(x)+1\\right)------\\{4\\}.$$\n",
        " A way of doing this is to use numerics to solve the fixed point equations. \n",
        "Choose a solver in your favourite programming language, and solve the fixed point equations numerically. Use $N=50$, $g=0.8$. *Hint: in Python, you can use the function root from the scipy.optimize package*.\n",
        "What is the value of the fixed point? Is the current variable $x_i^*$ for every neuron the same as in the simple fixed point? Print the histogram of entries $x_i^*$; then go back to $\\phi(x)=\\tanh(x)$, and plot the histogram again. Side note: is this the only fixed point? How would you test this numerically?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MswmI3E6f3aU"
      },
      "source": [
        "Complete the code below :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE8zclt23j1q"
      },
      "outputs": [],
      "source": [
        "#### #### #### #### #### #### ####\n",
        "np.random.seed(100)\n",
        "\n",
        "#### Find the fixed point\n",
        "\n",
        "#Define the non-linearity\n",
        "def Phi(x):\n",
        "  \"\"\"\n",
        "  Return the firing rate after calculating it \n",
        "  according to the formula described above in equation 4.\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  phi_x =\n",
        "  ### END CODE HERE ###\n",
        "  return phi_x\n",
        "  # return np.tanh(x)\n",
        "\n",
        "# This function finds the intersection\n",
        "def FixedPointEq(x,J):\n",
        "  \"\"\"\n",
        "  This function should return the equation 2 in proper form.\n",
        "  i.e ( by bringing all variables to one side )\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  out = \n",
        "  ### END CODE HERE ###\n",
        "  return out\n",
        "\n",
        "N = 50\n",
        "g = 0.8\n",
        "\n",
        "### START CODE HERE ###\n",
        "\"\"\"\n",
        "Initialize the variable \"initial_guess\" by sampling from a\n",
        "uniform distribution (for N neurons).\n",
        "\"\"\"\n",
        "initial_guess = \n",
        "\n",
        "# Initialize J the same way as before.\n",
        "J = \n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "\"\"\"\n",
        "optimize.root function approximates the roots of the objective function, starting with an initial guess,\n",
        "within the given tolerance/error (tol).objective function which in this case is FixedPointEq().\n",
        "\"\"\"\n",
        "sol = optimize.root(FixedPointEq, initial_guess, args=(J) ,tol=1e-10)\n",
        "\n",
        "#### #### #### #### #### #### ####\n",
        "#### Plot the histogram\n",
        "print(sol.x)\n",
        "fg = plt.figure()\n",
        "ax0 = plt.axes(frameon=True)\n",
        "\n",
        "plt.hist(sol.x)\n",
        "\n",
        "plt.xlabel(' Current to each neuron $x_i$')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIbguGdfiDPy"
      },
      "source": [
        "**g) Optional** The RNN dynamics that we have considered in this exercise and in the main lecture, which is written in terms of the input current variable $x_i(t)$, is only one among the two formalisms that are commonly used in the field. Another common choice is \n",
        "$$\\frac{d r_{i}(t)}{d t}=-r_{i}(t)+\\phi\\left(\\sum_{j=1}^{N} J_{i j} r_j(t)+h_{i}(t)\\right)------\\{5\\}$$\n",
        "where variable $r_i(t)$ is interpreted as the output firing rate of the neuron (instead of $\\phi(x_i)$).\n",
        "\n",
        " These two formalisms are in practice equivalent to each other, and are used indistinctly in the literature. \n",
        " \n",
        " If we consider constant external inputs, $h_{i}(t) = h_i$, there is a simple one-to-one mapping between both descriptions. Show that the two dynamics are exactly equivalent in this case: we can use a change of variable to transform one set of equations into the other. *Hint: define the variable $x_k = \\sum_{i=1}^N J_{ki} r_i + h_k$, and derive its time evolution...*\n",
        "\n",
        " The equivalence between both descriptions in the case of time-dependent  external inputs is discussed in this note: Miller & Fumarola (2012).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aowDp_WIL7S1"
      },
      "source": [
        "*Ans*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kxEhik019kh"
      },
      "source": [
        "\n",
        "For instance, the input current description is  used in the following studies: Sompolinsky, Crisanti & Sommers (1988),  Sussillo & Abbott. (2009), Laje & Buonomano (2013), Wang, Narain, Hosseini & Jazayeri (2018)..., while the rate-based description is used in: Wilson and Cowan (1972, 1973), Ben-Yishai, Hansel & Sompolinsky (1994), Dayan & Abbott book, Vogels, Rajan & Abbott (2005), Wong and Wang (2006),..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfIGiX2xt67H"
      },
      "source": [
        "# **##################### Problem 2 #####################**\n",
        "\n",
        "In Problem 1, we analyzed a non-linear random RNN by looking at its fixed points and their stability. We made the following predictions: (i) when $g<1$, dynamics converges towards a simple fixed point where $x_i=0$ $\\forall i$; (ii) when $g>1$, dynamics runs away from the fixed point. In this exercise, we use programming and numerical integration to characterize this transition, and test dynamics in both regimes.\n",
        "\n",
        "**a)** The code below builds and simulate a $N=500$ randomly-connected RNN defined as in the previous Problem. Each unit in the RNN also receives a periodic input of frequency $\\omega$\n",
        "$$\n",
        "h_i = I \\cos(\\omega t + \\theta_i)------\\{6\\}\n",
        "$$\n",
        "where phases $\\theta_i$ are distributed uniformely across neurons. To begin with, the value of $I$ is set to zero. \n",
        "\n",
        "The code simulates the RNN activity by integrating numerically the dynamics through Euler's method. Dynamics is simulated between $0$ and $t=T=100$. In order to apply Euler's method, we create a discretized time vector ($\\Delta t = 0.01$), and then use:\n",
        "$$x_{i,t+1} = x_{i,t} + \\Delta t \\frac{d x_{i,t}}{d t} = \n",
        "(1 - \\Delta t )x_{i,t} + \\Delta t \\left[ \\sum_{j=1}^{N} J_{i j} \\phi\\left(x_{j,t}\\right)+h_{i,t} \\right]------\\{7\\}.$$\n",
        "\n",
        "Take some time to familiarize with the code and make sure you understand every part after completing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyfBobrEiV0q"
      },
      "outputs": [],
      "source": [
        "np.random.seed(100)\n",
        "### Plot configuration\n",
        "\n",
        "fig_width = 4.2 # width in inches\n",
        "fig_height = 3.  # height in inches\n",
        "fig_size =  [fig_width,fig_height]\n",
        "plt.rcParams['figure.figsize'] = fig_size\n",
        "plt.rcParams['figure.autolayout'] = True\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "\n",
        "#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### ####\n",
        "#### Generate network\n",
        "\n",
        "N = 500      # Network size\n",
        "g = 0.8      # Connectivity strength\n",
        "I = 0.       # Input strength\n",
        "omega = 2.5  # Input frequency\n",
        "\n",
        "# Connectivity\n",
        "### START CODE HERE ###\n",
        "\n",
        "# Initialize J (the connectivity matrix) from a gaussian distribution \n",
        "J = \n",
        "\n",
        "# calculate eigen values of the matrix J\n",
        "eig = \n",
        "\n",
        "# Initialize input phases from a uniform distribution \n",
        "theta = \n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### ####\n",
        "#### Simulate activity \n",
        "\n",
        "# Function simulating one time step\n",
        "\n",
        "\n",
        "def sim_step (x_step, t_step):\n",
        "  \"\"\"\n",
        "  complete the function according to equation 7\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "\n",
        "  x_step = \n",
        "  \n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return x_step\n",
        "\n",
        "\n",
        "# Parameters\n",
        "\n",
        "T = 100                          # Total time\n",
        "dt = 0.01                        # Time step\n",
        "t = np.linspace(0, T, int(T/dt))      # Time array\n",
        "\n",
        "# Activity vector (time x neurons), initialize at random\n",
        "\n",
        "x = np.zeros(( len(t), N ))\n",
        "x[0,:] = np.random.normal(0, 1., N)\n",
        "\n",
        "# Simulate dynamics\n",
        "\n",
        "for it, ti in enumerate(t[:-1]):\n",
        "\tx[it+1,:] = sim_step(x[it,:], ti)\n",
        "\n",
        "r = np.tanh(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4qBL89xt67I"
      },
      "source": [
        "**b)** Set $I=0$. Now pick to values of $g$, respectively below and above 1. Simulate dynamics, and plot the firing rate activity $\\phi(x_j)$ of 3 sample neurons. Is activity in agreement with the prediction from Problem 1?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3iQBGUmt67I"
      },
      "outputs": [],
      "source": [
        "#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### ####\n",
        "#### Plot activity of the first 3 neurons\n",
        "\n",
        "fg = plt.figure()\n",
        "ax0 = plt.axes(frameon=True)\n",
        "\n",
        "plt.plot(t, r[:,0:3])\n",
        "\n",
        "plt.xlabel('time')\n",
        "plt.ylabel(r'activity $\\phi(x_j)$')\n",
        "\n",
        "plt.xlim(0, T)\n",
        "plt.ylim(-1.2, 1.2)\n",
        "plt.xticks([0, T//2, T])\n",
        "plt.yticks([-1.2, 0, 1.2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHYaFMzft67I"
      },
      "source": [
        "**c)** Now turn up the input amplitude $I$, first to 0.01 and then to 1, and see how the firing rates of the same 3 units in **b)** changed in response to the increasing strength of the drive.\n",
        "\n",
        "**d)** Calculate the covariance matrix of activity in each of the above scenarios: no input ( $I=0$ ), weak input ( $I=0.01$ ), and strong input ( $I=1$ ). The covariance matrix is mathematically defined as:\n",
        "$$\n",
        "C_{ij} = \\langle\\phi(x_i)\\phi(x_j)\\rangle - \\langle\\phi(x_i)\\rangle \\langle\\phi(x_j)\\rangle \n",
        "$$\n",
        "where the average is taken over time. \n",
        "Check to make sure that this comes out as an NxN matrix. Diagonalize this covariance matrix and project the activity onto a coordinate frame made up of the eigenvectors corresponding to the largest 3 eigenvalues of this covariance matrix. This is Principal Components Analysis; the eigenvectors of the covariance matrix gives you the PCs. If you did this correctly, congratulations, you have made your first State Space diagram! \n",
        "\n",
        "*Hint:  In Python, you can use from sklearn.decomposition import PCA, or compute the covariance matrix from a matrix multiplication.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu7EF4fkt67I"
      },
      "outputs": [],
      "source": [
        "#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### ####\n",
        "#### Do PCA\n",
        "C = np.zeros((N,N))\n",
        "\n",
        "### START CODE HERE ###\n",
        "\"\"\" Compute covariance matrix i.e C[i][j] for each i,j or compute\n",
        " it directly through matrix multiplication.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Compute eigenvectors\n",
        "\n",
        "V = \n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### ####\n",
        "#### Plot activity in the first 3 PC\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "# Plot every int_t = 10 steps\n",
        "int_t = 10\n",
        "ax.plot(np.dot(r[::int_t,:], V[:,-1]), np.dot(r[::int_t,:], V[:,-2]), np.dot(r[::int_t,:], V[:,-3]), '-o',color = '0.5')\n",
        "\n",
        "plt.rcParams['axes.labelpad'] = 0.1\n",
        "ax.dist = 12\n",
        "\n",
        "ax.grid('off')\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_zticks([])\n",
        "\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCsf6UIit67J"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RNN_assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}