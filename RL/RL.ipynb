{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"BiU-x0ujcRkE"},"outputs":[],"source":["# Copyright (c) 2020 Brain and Cognitive Society, IIT Kanpur [ BCS @IITK ]\n","# Copyright under MIT License, must reference https://github.com/bcs-iitk/BCS_Workshop_Apr_20 if used anywhere else.\n","# Author: Shashi Kant (http://shashikg.github.io/)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"l0QMGbtcQKNx"},"source":["## Reinforcement Learning\n","In this you have to implement and train an RL agent to find a path for a frozen lake problem. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qH7ubidoNaEv"},"source":["### Frozen Lake Problem Description:"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LTCY_Ip1M9Qc"},"source":["> Imagine there is a frozen lake stretching from your home to your office; you have to walk on the frozen lake to reach your office. But oops! There are holes in the frozen lake so you have to be careful while walking on the frozen lake to avoid getting trapped in the holes. [[src](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788836524/3/ch03lvl1sec32/solving-the-frozen-lake-problem)]\n","\n","![frozen-lake](https://static.packt-cdn.com/products/9781788836524/graphics/49f3e058-2f32-40e8-9992-b53d1f57d138.png)\n","\n","\n","Two task you have to do here:\n","\n","*   Implement a frozen lake scenario given the inputs, number of holes (M) and size of the lake (N) (Assume the lake is square). Starting point will be (0, 0) and goal will be to reach at (N-1, N-1)\n","*   Implemenat Q-learning method to learn a path from start to goal.\n","*   Use the following reward scheme: 50 points on reaching the goal, -50 points on stepping on a hole.\n","\n","#### Q-learning\n","Recall from the lecture video that `Q[state, action]` gives you an action state pair to get an optimal policy. Recall the Q-Loss from the lecture video i.e:\n","> $E = ||r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a)||^2$\n","\n","Use gradient descent to minimise $E$ and work out a learning rule for $Q(s, a)$. \n","> Take $\\max_{a'} Q(s', a')$ and $r$ to be independent of $Q(s, a)$.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pbkIHLK7NFEw"},"source":["### Defining important functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"QnnNULcYQNRj"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"n9vGeSBZEG-r"},"outputs":[],"source":["ActionMap = ['Up', 'Right', 'Down', 'Left']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"izQzYoXBHVgr"},"outputs":[],"source":["def get_board(N, M):\n","  # should return an N x N size frozen lake - board with M randomle placed holes.\n","  # use 'S' representation for starting point\n","  # use 'G' representation for goal point\n","  # use 'H' representation for holes\n","  # use 'F' for frozen lakes\n","  # use 'C' for displaying agents current position on the board.\n","  # Refer the representation from the image shown above\n","\n","  # Write your code here ----------\n","\n","  # -------------------------------\n","\n","  return board"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"nu6PW4KhIZH7"},"outputs":[],"source":["def get_reward(board, N, M):\n","  # should return an N x N size reward table for the generated frozen lake scenario\n","  # use 50 reward for 'G' point\n","  # use -50 reward for 'H' point\n","  # o for rest.\n","\n","  # Write your code here ----------\n","\n","  # -------------------------------\n","\n","  return reward"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"B1ueYhO8Ecnh"},"outputs":[],"source":["class FrozenLake:\n","  def __init__(self, N, M):\n","    # Recall python class, this function is called when you first initialise the class\n","    # Should intialise the board and reward table based on the reward scheme\n","    # Select M numbers of holes randomly\n","\n","    self.board = get_board(N, M)\n","    self.init_board = copy.deepcopy(self.board)\n","    self.reward = get_reward(self.board, N, M)\n","    self.state = (0, 0)\n","    self.finish = 0\n","    self.N = N\n","\n","\n","  def reset(self):\n","    # should reset the env with board to initial state\n","    # hint: set self.state at 0, 0 and use self.init_board to reset self.board\n","\n","    # Write your code here ----------\n","\n","    # -------------------------------\n","    self.finish = 0\n","\n","    return self.state\n","  \n","  def step(self, action):\n","    # ===== Action Table =========\n","    #     0 -- UP\n","    #     1 -- RIGHT\n","    #     2 -- DOWN\n","    #     3 -- LEFT\n","    # perform the given action and get update the  self.state, get reward, and update the self.board according to new state\n","    # update the self.board means update the new position with 'C' and replace previous position with {'S', 'F', 'G'} which is actually there according to the self.init_board\n","\n","    # Write your code here ----------\n","\n","    # -------------------------------\n","    \n","    # status to check if you reached your goal\n","    if self.state == (N-1, N-1):\n","        self.finish = 1\n","        \n","    return self.state, reward, self.finish\n","  \n","  def get_random_action(self):\n","    # ===== Action Table =========\n","    #     0 -- UP\n","    #     1 -- RIGHT\n","    #     2 -- DOWN\n","    #     3 -- LEFT\n","    # should return a possible random action out of the four\n","    # hint: note that when you are around the corner or sides of the board not all four action will be available for you\n","\n","    # Write your code here ----------\n","\n","    # -------------------------------\n","\n","    return action    \n","      \n","  def display(self):\n","    print(self.board)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oizARLSBNMxB"},"source":["### Environment creation and learning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"yGXRU7ImRnT6"},"outputs":[],"source":["def explore_exploit(env, Q, state, episode):\n","  # Notice that if you always select your new action based on maximum Q-value you will never get to see any new path right?\n","  # You have to explore the environment to know new paths\n","  # Write your code here to randomly select whether you want to explore or exploit\n","  # The probability of exploration should be exp(-episode*5e-4)\n","  # for exploration get some random action\n","  # for exploitation get action based on max Q value\n","\n","  # Write your code here ----------\n","\n","  # -------------------------------\n","\n","  return action"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"N6ym5P3-LxOz"},"outputs":[],"source":["def init_env_and_learn(N=6, M=12, gamma=0.8, lr=0.8):\n","  # gamma: gamma param of total discounted reward\n","  # lr: learning rate for Q updates\n","  # N = grid size of frozen lake wil be N x N\n","  # M = Number of holes\n","  # returns env, Q-function, rewards\n","\n","  env = FrozenLake(N, M)\n","  Q = np.zeros((N, N, 4))\n","\n","  total_episodes = 3000 # i.e. the number of times your RL agent will run through the board.\n","  max_steps = N*N*3 # maximum number of steps to perform\n","\n","  rewards = []\n","  for episode in range(total_episodes):\n","    state = env.reset()\n","    total_rewards = 0\n","\n","    for step in range(max_steps):\n","      action = explore_exploit(env, Q, state, episode)\n","\n","      # Write your code here ----------------------------------------------------------\n","      # Should perform the action get reward, new_state, finish status and update the Q value\n","\n","\n","      reward = '...' # replace these with your values\n","      finish = '...' # replace these with your values\n","      new_state = '...' # replace these with your values\n","      # -------------------------------------------------------------------------------\n","\n","      total_rewards += reward\n","      state = new_state\n","      \n","      if finish == 1: \n","          break\n","\n","    rewards.append(total_rewards)\n","\n","  return env, Q, rewards"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"6z8EuJ6sXCTK"},"outputs":[],"source":["def travel_path(env, Q):\n","  # write a function to display a sequence of path performed using the learned Q-values\n","  # show initial and final frozen lake board \n","  # to perform an action at a state simply take max of Q at that state\n","\n","  # Write your code here ----------\n","\n","  # -------------------------------\n","\n","  return"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ios3sB8ZYDTN"},"source":["### Use N = 6, M = 10 and learn the models for following sets of gamma and lr:\n","\n","*   `(gamma, lr) = (0.8, 0.8)`\n","*   `(gamma, lr) = (0.95, 0.8)`\n","*   `(gamma, lr) = (0.6, 0.8)`\n","*   `(gamma, lr) = (0.8, 0.95)`\n","*   `(gamma, lr) = (0.8, 0.1)`\n","\n","Plot rewards vs episode for each of them and compare.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"1BB8d1_3MhsL"},"outputs":[],"source":["# Write your code here ----------\n","\n","# -------------------------------\n","\n","print(\"Replace this with your observation\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yI5oHjVbZkIW"},"source":["### Use N = 6, M = 10 and learn a models with (gamma, lr) = (0.8, 0.8)\n","After learning the model, display a path traveled from source to goal."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"RtDOaUEeMi-0"},"outputs":[],"source":["# Write your code here ----------\n","\n","# -------------------------------"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPffDXWPf7sDUd9XWqUmhTa","collapsed_sections":[],"name":"ML.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
